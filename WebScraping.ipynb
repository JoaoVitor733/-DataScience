{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WebScraping.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNmSs1VoxpuCiG4MJAfVwjr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoaoVitor733/-DataScience/blob/Master/WebScraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SNbkmE1Z-GGg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy as np \n",
        "\n",
        "import requests as rq \n",
        "from bs4 import BeautifulSoup \n",
        "import re  "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "link= '/filmes/filme-176535/' #primeira página a ter dados coletados\n",
        "\n",
        "info_livros = [] #vetor para armazenar todas informações das páginas análisadas\n",
        "linksUsados = [] #vetor com todos os links usados\n",
        "linksPodemSerUsados = [] #vetor com os links disponíveis\n",
        "\n",
        "linksPodemSerUsados.append(link)\n",
        "\n",
        "for row in linksPodemSerUsados: #andando no vetor dos links\n",
        "  if len(linksUsados)> 1000: #analisando apenas uma certa quantidade de páginas\n",
        "      break\n",
        "\n",
        "  livro = {} #guardará os dados de cada página\n",
        "  \n",
        "  try:\n",
        "    \n",
        "    listaAux = []\n",
        "    \n",
        "    url = 'https://www.adorocinema.com{}'.format(row) #pega o link raiz mais o link da vez\n",
        "    paginaBruta = rq.get(url) #pega todo texto da página\n",
        "    paginaParsada = BeautifulSoup(paginaBruta.text ,'html.parser') #cria um objeto capaz de acessar as tags HTML\n",
        "\n",
        "    livro['Titulo: '] = paginaParsada.find('div',{'class':'titlebar-title titlebar-title-lg'}).get_text() #pega a primeira tag com esse atributo e retorna o texto\n",
        "\n",
        "    nacionalidade = paginaParsada.find('div',{'class':'item'}).get_text()\n",
        "    nacionalidade = nacionalidade.split('\\n') #retirando os \\n presente no texto retornado\n",
        "    cont = 0\n",
        "    #tratando o texto com uma lista auxiliar para pegar apenas as nacionalidades\n",
        "    for x in nacionalidade:\n",
        "      if not(x == ''):\n",
        "        if cont > 0:\n",
        "           listaAux.append(x)\n",
        "        cont += 1\n",
        "    livro['Nacionalidade: '] =  str(listaAux)\n",
        "\n",
        "    divDados = paginaParsada.find('div',{'class':'meta-body'})\n",
        "    \n",
        "    livro['Duração: '] = divDados.find('div',{'class':'meta-body-item meta-body-info'}).text.split('/')[1].split('\\n')[1] #retirando os / e \\n\n",
        "    \n",
        "    genero = divDados.find('div',{'class':'meta-body-item meta-body-info'}).text.split('/')[2].split('\\n') #retirando os / e \\n\n",
        "    for x in genero: #retirando os espaços em brancos\n",
        "      if x == '':\n",
        "        genero.remove(x) \n",
        "    livro['Gênero: '] = genero\n",
        "\n",
        "    livro['Direção: '] =  paginaParsada.find('div',{'class':'meta-body-item meta-body-direction'}).get_text().split('\\n')[2]\n",
        "\n",
        "    livro['Sinopse: '] = paginaParsada.find('div',{'class':'content-txt '}).get_text().replace('\\n','').split('.\\xa0')[0]#trocando \\n por espaço vazio e retirando o .\\xa0\n",
        "    \n",
        "    secaoFilmes = paginaParsada.find('div',{'class','gd gd-gap-20 gd-xs-2 gd-s-4 last-main-row'})\n",
        "    link = secaoFilmes.find_all('a',{'class','meta-title-link'})\n",
        "    for x in range(4):\n",
        "       testarLink = str(link[x]).split('href=\"')[1].split('\" title=')[0]\n",
        "       linksPodemSerUsados.append(testarLink) #pegando os 4 links de filmes recomendados de cada página\n",
        "  \n",
        "    \n",
        "    info_livros.append(livro)  #coloca na lista as informações de cada página\n",
        "    linksUsados.append(link) #armazenando os links ja usados\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e)\n"
      ],
      "metadata": {
        "id": "4dkgL2fz-fQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(info_livros) #criando um data frame\n",
        "df.to_csv('./Informacoes.csv') #criando um arquivo csv"
      ],
      "metadata": {
        "id": "2KE_iNSs_YHh"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "cuDbH7AkFIAd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}